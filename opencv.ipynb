{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "309c298e",
      "metadata": {
        "id": "309c298e"
      },
      "outputs": [],
      "source": [
        "#pip install flask\n",
        "#uncomment the above line to install flask framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb431c51",
      "metadata": {
        "id": "cb431c51"
      },
      "outputs": [],
      "source": [
        "#importing all the important libraries for our code\n",
        "from flask import Flask, render_template, Response\n",
        "import tkinter as tk\n",
        "from tkinter import *\n",
        "from PIL import Image, ImageTk\n",
        "import numpy as np\n",
        "import random \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob as gb\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\n",
        "from keras.models import Model,Sequential\n",
        "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
        "import time\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f4ca6e1",
      "metadata": {
        "id": "4f4ca6e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "cf029d81-8733-4d2d-a26b-3eb4e44a0b7d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-e8a2f6f479ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#loading our saved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0memotion_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"C:\\Users\\nikks\\Downloads\\research50.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# if you want vgg16 model then uncomment below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#emotion_model = load_model(r'C:\\Users\\nikks\\Downloads\\model.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'No file or directory found at {filepath_str}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at C:\\Users\\nikks\\Downloads\\research50.h5"
          ]
        }
      ],
      "source": [
        "#loading our saved model\n",
        "emotion_model = load_model(r'C:\\Users\\nikks\\Downloads\\research50.h5')\n",
        "# if you want vgg16 model then uncomment below \n",
        "#emotion_model = load_model(r'C:\\Users\\nikks\\Downloads\\model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d69d4a3",
      "metadata": {
        "id": "8d69d4a3"
      },
      "outputs": [],
      "source": [
        "# initializing flask application\n",
        "app=Flask(__name__)\n",
        "\n",
        "# getting camera view\n",
        "cap=cv2.VideoCapture(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80e21ce9",
      "metadata": {
        "id": "80e21ce9"
      },
      "outputs": [],
      "source": [
        "#defining function that returns stream of bytes of frame to flask call\n",
        "def gen():\n",
        "    cv2.ocl.setUseOpenCL(False)\n",
        "    #emotion dictionary with key as the last layer of deep neural network, and value as the corresponding emotion\n",
        "    #emotion_dict={0:\"Angry\", 1:\"Disgusted\", 2:\"Fearful\", 3:\"Happy\", 4:\"Neutral\", 5:\"Sad\", 6:\"Surprised\"}\n",
        "    emotion_dict={0:r\"C:\\Users\\nikks\\Downloads\\angry.jpg\", 1:r\"C:\\Users\\nikks\\Downloads\\disgusted.jpg\", 2:r\"C:\\Users\\nikks\\Downloads\\fearful.jpg\", 3:r\"C:\\Users\\nikks\\Downloads\\happy.jpg\", 4:r\"C:\\Users\\nikks\\Downloads\\neutral.jpg\", 5:r\"C:\\Users\\nikks\\Downloads\\sad.jpg\", 6:r\"C:\\Users\\nikks\\Downloads\\surprise.jpg\"}\n",
        "\n",
        "    while True:\n",
        "        ret, frame= cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        bounding_box= cv2.CascadeClassifier(r'C:\\Python310\\Lib\\site-packages\\cv2\\data\\haarcascade_frontalface_default.xml')\n",
        "        gray_frame=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        num_faces=bounding_box.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
        "        for (x, y, w, h) in num_faces:\n",
        "            cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
        "            roi_gray_frame=gray_frame[y:y+h, x:x+w]\n",
        "            cropped_img=np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48,48)), -1), 0)\n",
        "\n",
        "            #getting array of seven values, each value representing relative probability of a particular emotion of being the result\n",
        "            emotion_prediction=emotion_model.predict(cropped_img)\n",
        "\n",
        "            #getting the index of maximum probable emotion\n",
        "            maxindex=int(np.argmax(emotion_prediction))\n",
        "\n",
        "            #reading the emoji corresponding to maxindex from emotion dictionary\n",
        "            em_img=cv2.imread(emotion_dict[maxindex])\n",
        "\n",
        "            #getting dimensions of image and adding to the video frame\n",
        "            img_height,img_w,_=em_img.shape\n",
        "            y=y-50\n",
        "            frame[y:y+img_height,x:x+img_w]=em_img\n",
        "\n",
        "            #breking the video input frame to bytes of information to be returned to flask call\n",
        "            ret,buffer=cv2.imencode('.jpg',frame)\n",
        "            frame=buffer.tobytes()\n",
        "            yield (b'--frame\\r\\n'\n",
        "                   b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n",
        "            # to get stable result giving a time lag of .1 second\n",
        "            time.sleep(.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1de54c0",
      "metadata": {
        "id": "c1de54c0"
      },
      "outputs": [],
      "source": [
        "@app.route('/')\n",
        "def home():\n",
        "  #error handling if the homepage of html file is not shown up\n",
        "    try:\n",
        "        return render_template(r'C:\\Users\\nikks\\Downloads\\mainpage.html')#render template helps in showing a separate html file in flask\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "@app.route('/d')\n",
        "def d():\n",
        "    return Response(gen(), mimetype='multipart/x-mixed-replace; boundary=frame')# Response helps in returning stream of bytes of frame\n",
        "if __name__==\"__main__\":\n",
        "  #running application\n",
        "    Flask.run(app)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af4c2a65",
      "metadata": {
        "id": "af4c2a65"
      },
      "outputs": [],
      "source": [
        "#code to check emotion to text\n",
        "\n",
        "#  cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "# emotion_dict={0:\"Angry\", 1:\"Disgusted\", 2:\"Fearful\", 3:\"Happy\", 4:\"Neutral\", 5:\"Sad\", 6:\"Surprised\"}\n",
        "\n",
        "#  cap=cv2.VideoCapture(0)\n",
        "\n",
        "#  while True:\n",
        "#      ret, frame= cap.read()\n",
        "#      if not ret:\n",
        "#          break\n",
        "#      bounding_box= cv2.CascadeClassifier(r'C:\\Users\\AMARTY JHA\\anaconda3\\Lib\\site-packages\\cv2\\data\\haarcascade_frontalface_default.xml')\n",
        "#      gray_frame=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "#      num_faces=bounding_box.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
        "#      for (x, y, w, h) in num_faces:\n",
        "#          cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
        "#          roi_gray_frame=gray_frame[y:y+h, x:x+w]\n",
        "#          cropped_img=np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48,48)), -1), 0)\n",
        "#          emotion_prediction=emotion_model.predict(cropped_img)\n",
        "#          maxindex=int(np.argmax(emotion_prediction))\n",
        "#          cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "#      cv2.imshow('Video', cv2.resize(frame, (1200, 860), interpolation=cv2.INTER_CUBIC))\n",
        "#      if cv2.waitKey(1)&0xFF==ord('q'):\n",
        "#          cap.release()\n",
        "#          cv2.destroyAllWindows()\n",
        "#          break;"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "test (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}